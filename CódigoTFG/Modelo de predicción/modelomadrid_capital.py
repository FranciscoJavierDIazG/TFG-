# -*- coding: utf-8 -*-
"""ModeloMadrid_Capital.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_YVN7rvOCuTkoplGwgwqfrjbpz4uT4gs
"""

# !pip3 install pandas
# !pip3 install tensorflow
# !pip3 install matplotlib
# !pip3 install io
# !pip3 install sklearn

import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import io
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.utils import resample

# from google.colab import drive
# drive.mount('/content/drive')

#Cargo los datos para entrenar
train_data = pd.read_csv('/home/usr_1ctdiazfranciscojavier_gmail/Data.txt',sep=';',encoding='latin-1', decimal=",")

#Muestro las 5 primeras filas para comprobar que se ha cargado correctamente
train_data.head()

df_majority = train_data[train_data.Ocupada==0]
df_minority = train_data[train_data.Ocupada==1]

df_majority

# Realizo un upsampling de los datos que están ocupados para que estén balanceados los datos de ocupados y no ocupados
df_minority_upsampled = resample(df_minority,
                                 replace = True,
                                 n_samples=96900,
                                 random_state=123)

df_minority_upsampled

train_data=pd.concat([df_majority, df_minority_upsampled])

train_data = train_data.sample(frac=1).reset_index(drop=True)

train_data = train_data.head(100000)

#Escojo la variable a predecir
train_labels = train_data['Ocupada']

#Selecciono las variables importantes para predecir
columns_to_extract = ['Calle','Localización',	'Habitantes','Paro_registrado','N_detenciones',
                      'Detenciones/Habitantes','Viviendas vacías','Renta media por persona',
                      'N_extranjeros','Propietario',
                      'Alarma']

#Creo un dataframe con las variables con las que voy a predecir
train_features = train_data[columns_to_extract]

#Muestro las 5 primeras filas por pantalla
train_features.head()

# Voy a coger por separado las variables numéricas y las categóricas para tratarlas más fácilmente
# Creo el dataframe de las variables numéricas -->  var_num
var = ['Habitantes','Paro_registrado','N_detenciones',
       'Detenciones/Habitantes','Viviendas vacías','Renta media por persona',
       'N_extranjeros']
var_num=train_features[var]
var_num.head()

# Creo el dataframe de las variables categóricas -->  var_cat
var2 = ['Calle','Localización','Propietario','Alarma']
var_cat=train_features[var2]
var_cat.head()

#Creo una función para normalizar mis valores de las variables numéricas --> norm(df)
def norm(df):
      return (df - df.min()) / ( df.max() - df.min())

#Normalizo los valores de las variables numéricas
var_num = norm(var_num)

#Muestro el resultado para comprobar que lo ha normalizado
var_num.head()

# Optimizo las variables categóricas con la función get_dummies, con la que asigna 0 o 1 en las casillas correspondientes
var_cat = pd.get_dummies(var_cat, columns=['Alarma'] ,drop_first=True)

# Hago lo mismo con la variable -- Localización -- pero a parte porque al tener 21 entradas, no se podía dejar en una sola columna
var_cat = pd.get_dummies(var_cat, columns=['Calle','Localización','Propietario'])

#Muestro el resultado
var_cat.head()

# Uno los dos dataframes, el de variables numéricas y categóricas
train_features = pd.concat([var_num,var_cat],axis=1,sort=False)

#Muestro el resultado
train_features.head()

len(train_features.columns)

vector_size = len(train_features.columns)

# Defino mi red neuronal
# Voy a crear una red con 3 capas:
      # Capa 1 --> 10 neuronas
      # Capa 2 (oculta)--> 3 neuronas
      # Capa 3 --> 1 neurona (ya que la salida va a ser binaria 0 ó 1)

# Para solventar el problema del Overfitting --> utilizo .Dropout que lo que hace es desactivar neuronas de forma aleatoria

model = tf.keras.Sequential([
    tf.keras.layers.Input((None,vector_size)),
    #Capa 1
    tf.keras.layers.Dense(10, activation='relu'),

    #Overfitting
    tf.keras.layers.Dropout(0.2),
    #Capa 2
    tf.keras.layers.Dense(3, activation='relu'),

    # Capa 3
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.binary_crossentropy,
              metrics=['accuracy'])

# Entreno el modelo
#Separo los datos en 30% de Test - 70% de Training
history = model.fit(train_features,train_labels,validation_split=0.3, epochs=15)

# model.save('/home/usr_1ctdiazfranciscojavier_gmail')

# Muestro las variables de evaluación del model -- loss -- y -- accuracy--
test_loss, test_acc = model.evaluate(train_features, train_labels,verbose=2)

print(history.history.keys())

# Dibujo las gráficas de la evolución de las evaluaciones por cada epoch
# El naranja corresponde a los valores del test y el azul al train

# Accuracy (Precisión)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'],loc = 'upper left')
plt.show()

# Loss (Pérdida)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'],loc = 'upper left')
plt.show()

model.save('/home/usr_1ctdiazfranciscojavier_gmail/MODELO_FINAL')

# Importo los datos de mi dataset para evaluarlo
data_test = pd.read_csv('/home/usr_1ctdiazfranciscojavier_gmail/Test.txt',sep=';',encoding='latin-1', decimal=",")

# Lo visualizo
data_test.head()

#Selecciono las variables importantes para predecir
columns_to_extract2 = ['Calle','Localización',	'Habitantes','Paro_registrado','N_detenciones',
                      'Detenciones/Habitantes','Viviendas vacías','Renta media por persona',
                      'N_extranjeros', 'Propietario',
                      'Alarma']

#Creo un dataframe con las variables con las que voy a predecir
test_features = data_test[columns_to_extract2]
#Muestro las 5 primeras filas por pantalla
test_features.head()

# Voy a coger por separado las variables numéricas y las categóricas para tratarlas más fácilmente
# Creo el dataframe de las variables numéricas -->  var_num
var_t = ['Habitantes','Paro_registrado','N_detenciones',
       'Detenciones/Habitantes','Viviendas vacías','Renta media por persona',
       'N_extranjeros']
var_num_t=test_features[var_t]
var_num_t.head()

# Creo el dataframe de las variables categóricas -->  var_cat
var2_t = ['Calle','Localización','Propietario','Alarma']
var_cat_t=test_features[var2_t]
var_cat_t.head()

#Normalizo los valores de las variables numéricas
var_num_t = norm(var_num_t)

#Muestro el resultado para comprobar que lo ha normalizado
var_num_t.head()

# Optimizo las variables categóricas con la función get_dummies, con la que asigna 0 o 1 en las casillas correspondientes
var_cat_t = pd.get_dummies(var_cat_t, columns=['Alarma'] ,drop_first=True)

# Hago lo mismo con la variable -- Localización -- pero a parte porque al tener 21 entradas, no se podía dejar en una sola columna
var_cat_t = pd.get_dummies(var_cat_t, columns=['Calle','Localización','Propietario'])

#Muestro el resultado
var_cat_t.head()

#Uno los 2 dataframes de variables (categóricas y numéricas)
test_features = pd.concat([var_num_t,var_cat_t],axis=1,sort=False)
test_features.head()

print(len(test_features.columns))
print(len(test_features))

tam = vector_size-len(test_features.columns)
cont=0
while cont<tam:
  name = 'Calle_'+ str(cont)
  test_features.insert(0,name,0, allow_duplicates=False)
  cont+=1

len(test_features.columns)

test_features

# test_features.to_csv('test_procesado.txt', index=False)

# Realizo las predicciones de mi test
predictions = model.predict(test_features)

# Imprimo los resultados de las predicciones
print(predictions)

# Redondeo los resultados para que sean 1 (Ocupada) y 0 (NO ocupada)

# Lista donde almacenaré mis predicciones
out = []
for i in predictions:
  # Como los resultados son tan próximos a uno, establezco mi umbral en 0.7 y me quedo con los más cercanos a 1
  if i>=0.7:
    out.append(1)
  else:
    out.append(0)

# Creo un dataframe con el ID de cada vivienda, su calle y localización, y el resultado de si va a ser ocupada
result = pd.concat([data_test['ID'],data_test['Calle'],data_test['Localización'],pd.DataFrame(data=out,columns=['Ocupada'])],axis=1,sort=False)

# Muestro el resultado para comprobarlo
result

# Guardo el dataframe resultante en un csv
result.to_csv('result2.txt', index=False)